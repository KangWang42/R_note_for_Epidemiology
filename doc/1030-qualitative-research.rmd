---
title: "质性研究与文本挖掘 (Text Mining) 完全指南"
date: "2026-01-13"
categories: [R包, 质性研究, 文本挖掘]
image: "images/1030_cover.png"
description: "如何使用 R 语言的 tidytext 和 LDA 主题模型辅助质性研究，发现文本背后的模式。"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    warning = FALSE,
    message = FALSE,
    fig.width = 8,
    fig.height = 6,
    dpi = 300
)
```

## 1. 为什么质性研究需要 R？

**质性研究 (Qualitative Research)** 传统上依赖人工阅读、编码和归纳（如使用 NVivo, MAXQDA）。但在面对**海量文本**（如成千上万条患者评论、政策文档）时，人工编码变得不可能。

**文本挖掘 (Text Mining)** 提供了“远读 (Distant Reading)”的视角，帮助我们：
-   **三角验证 (Triangulation)**：用量化结果验证质性发现。
-   **探索性分析**：快速识别高频词和潜在主题。
-   **情感分析**：量化文本的情感倾向。

## 2. 工具准备

我们将使用 `tidytext` 框架，它将文本处理融入到熟悉的 Tidyverse 工作流中。

-   `tidytext`: 文本整洁化。
-   `jiebaR`: 中文分词神器（*注：本环境暂不可用，以下仅展示代码逻辑*）。
-   `topicmodels`: LDA 主题模型。
-   `wordcloud2`: 交互式词云。

> [!WARNING]
> 由于 `jiebaR` 包在当前 Windows R 4.5 环境中兼容性问题，以下涉及中文分词的代码块已设置为不执行 (`eval=FALSE`)。请在本地安装了 Rtools 的环境中尝试运行。

## 3. 安装与加载

```{r eval=FALSE}
install.packages(c("tidytext", "jiebaR", "topicmodels", "wordcloud2", "dplyr", "ggplot2"))
```

```{r}
library(tidytext)
# library(jiebaR)
library(dplyr)
library(ggplot2)
library(wordcloud2)
```

## 4. 中文分词机制

中文与英文不同（英文有空格分隔），需要专门的算法切分词语。

```{r eval=FALSE}
# 模拟访谈数据
text_data <- data.frame(
    id = 1:3,
    text = c(
        "患者表示对治疗效果非常满意，护士服务态度很好。",
        "治疗过程很痛苦，但是为了康复必须坚持，医生很专业。",
        "医院环境不错，但是挂号排队时间太长了，希望改进。"
    ),
    stringsAsFactors = FALSE
)

# 初始化分词器 (混和模式)
cutter <- worker()

# 分词函数
segment_text <- function(text) {
    segment(text, cutter)
}

# 1. 列表列转换：将句子切分为词向量
text_data$segmented <- sapply(text_data$text, function(x) paste(segment_text(x), collapse = " "))

# 2. Tidy 格式转换 (Unnest)
# 这一步是核心：将 "一行一文档" 转换为 "一行一词 (One-token-per-row)"
tidy_text <- text_data %>%
    unnest_tokens(word, segmented, token = "ngrams", n = 1)

head(tidy_text)
```

**Tidy Text 格式示例：**
转换后的数据结构如下，非常适合 `dplyr` 处理：

| id | word |
| -- | ---- |
| 1 | 患者 |
| 1 | 表示 |
| 1 | 满意 |
| 2 | 治疗 |

## 5. 词频统计与可视化

我们使用自带的英文数据集 `sentiments` 或模拟数据来演示流程（跳过分词步骤）。

```{r eval=FALSE}
# 统计高频词
word_counts <- tidy_text %>%
    count(word, sort = TRUE) %>%
    filter(nchar(word) > 1) # 过滤掉单字（如"的"、"了"）

# 柱状图展示
word_counts %>%
    head(10) %>%
    ggplot(aes(reorder(word, n), n)) +
    geom_col(fill = "#4e79a7") +
    coord_flip() +
    labs(x = NULL, y = "出现频次", title = "访谈核心议题") +
    theme_minimal(base_size = 14)
```

## 6. LDA 主题模型 (Latent Dirichlet Allocation)

LDA 是一种非监督学习算法，它假设：
1.  每篇文档是由多种**主题**混合而成的。
2.  每个主题是由多种**词汇**混合而成的。

它的目标是反推这些“主题”。

```{r eval=FALSE}
library(topicmodels)

# 1. 构建文档-词矩阵 (Document-Term Matrix, DTM)
# 这是 LDA 的标准输入格式
dtm <- tidy_text %>%
    count(id, word) %>%
    cast_dtm(id, word, n)

# 2. 运行模型 (假设存在 K=2 个主题)
lda_model <- LDA(dtm, k = 2, control = list(seed = 1234))

# 3. 提取 "词-主题" 概率 (beta)
topics <- tidy(lda_model, matrix = "beta")

# 4. 可视化每个主题的关键词
top_terms <- topics %>%
    group_by(topic) %>%
    slice_max(beta, n = 5) %>%
    ungroup() %>%
    arrange(topic, -beta)

top_terms %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~topic, scales = "free") +
    scale_y_reordered() +
    labs(
        title = "LDA 模型提取的潜在主题",
        subtitle = "主题 1 可能对应 '就医体验'，主题 2 对应 '治疗感受'",
        x = "Beta (词属于该主题的概率)", y = NULL
    )
```

## 7. 混合研究设计的应用

将文本挖掘结果整合回质性研究：

1.  **关键词导出**：将 LDA 提取的 Top 20 词汇作为 NVivo 的**预设节点 (Nodes)**。
2.  **情感量化**：计算每个受访者的情感得分，与他们的问卷满意度进行相关性分析（Mixed Methods）。
3.  **异常值发现**：找到不符合主流主题的“离群”文档，进行深入个案研究。

## 参考文献

-   Silge, J., & Robinson, D. (2017). *Text Mining with R: A Tidy Approach*. O'Reilly Media.
-   Blei, D. M. (2012). Probabilistic topic models. *Communications of the ACM*.
