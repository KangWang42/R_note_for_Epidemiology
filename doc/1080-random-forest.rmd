---
title: 随机森林与决策树完整教程
subtitle: "从可解释树模型到稳健的森林集成"
date: "2026-01-17"
image: images/1080-random-forest-cover.svg
categories:
- 机器学习与AI
- 机器学习框架
- 树模型
- 随机森林
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 7.5,
  fig.height = 5,
  dpi = 150,
  out.width = "100%",
  fig.path = "figure/1080-random-forest-"
)
```

```{r packages}
library(dplyr)
library(ggplot2)
library(rsample)
library(yardstick)
library(rpart)
library(rpart.plot)
library(ranger)
```

## 教程目标与适用场景

决策树提供清晰的规则解释，随机森林通过集成提升稳健性。
本教程展示从单棵树到森林模型的完整训练、调参与解释流程。

**适合场景**

- 特征与结局之间存在非线性或交互效应
- 需要兼顾预测效果与一定解释性
- 变量尺度不同但希望减少复杂预处理

**不适合场景**

- 高维稀疏文本数据（需更适配的模型）
- 要求完整可解释的线性关系场景

## 模型入门：从0理解决策树与随机森林

决策树把建模过程理解为一系列“如果-那么”规则：在每一步选一个特征，
将样本分成更接近同一类别的小组，直到叶子节点足够纯净。
它的优势是直观、可解释，缺点是容易过拟合，且对训练数据波动敏感。

随机森林的核心思想是“多棵树的投票”。训练时对样本做自助抽样，
并在每个节点只随机抽取部分特征参与分裂，形成许多结构不同的树。
最终分类结果取多数投票，回归取平均，从而显著降低方差。

在实践中可以把它理解为：先用决策树建立最容易讲清楚的基线，
再用随机森林提升稳定性，同时用特征重要性解释模型决策。

## 方法框架与流程

决策树与随机森林的完整流程可以分成四个阶段：
先明确任务类型与评价指标，再进行数据划分与特征准备；
接着用单棵树建立可解释基线，最后用随机森林提升泛化表现并做解释。
这个顺序可以避免先上复杂模型而忽略数据与评价口径。

### 关键术语与分裂准则

决策树的“好分裂”本质是让节点更纯：
分类任务常用基尼指数或信息增益，回归任务常用均方误差。
随机森林通过“样本自助抽样 + 特征子集随机化”降低树之间的相关性，
从而减少模型方差并提升泛化能力。

### 关键超参数与影响

- `cp`（复杂度参数）：控制是否继续分裂，数值越大树越浅。
- `mtry`：每次分裂可用的特征数，越小随机性越强。
- `num.trees`：树的数量，越多越稳定，但训练时间增加。
- `min.node.size`：叶子节点最小样本数，过小容易过拟合。

### 评价指标选择

本教程用二分类示例，因此关注准确率与 ROC AUC。
准确率直观但受阈值影响，ROC AUC 更能反映区分度。
实际应用中应结合业务成本设定阈值与评价优先级。

## 数据准备与任务定义

这里用 Iris 数据构造二分类任务：`setosa` vs 其他类别。

```{r data-prep}
set.seed(2026)
iris_binary <- iris |>
  mutate(target = if_else(Species == "setosa", "setosa", "other")) |>
  select(-Species) |>
  mutate(target = factor(target, levels = c("setosa", "other")))

split_obj <- initial_split(iris_binary, prop = 0.7, strata = target)
train_data <- training(split_obj)
test_data <- testing(split_obj)
```

### 基线：决策树模型

```{r decision-tree-fit}
tree_model <- rpart(
  target ~ ., 
  data = train_data,
  method = "class",
  control = rpart.control(cp = 0.01)
)

rpart.plot(tree_model, type = 4, extra = 104)
```

```{r decision-tree-predict}
tree_prob <- predict(tree_model, test_data, type = "prob")
tree_pred <- predict(tree_model, test_data, type = "class")

tree_metrics <- tibble(
  truth = test_data$target,
  .pred_setosa = tree_prob[, "setosa"],
  .pred_class = tree_pred
) |>
  summarise(
    accuracy = yardstick::accuracy_vec(truth, .pred_class),
    roc_auc = yardstick::roc_auc_vec(truth, .pred_setosa, event_level = "first")
  )

tree_metrics
```

## 随机森林模型

### 训练基础随机森林

```{r rf-fit}
set.seed(2026)
rf_model <- ranger(
  target ~ .,
  data = train_data,
  num.trees = 500,
  mtry = 2,
  min.node.size = 3,
  probability = TRUE,
  importance = "permutation"
)
```

随机森林可直接使用 OOB（袋外）误差作为泛化评估，
在样本量有限时比单独划分验证集更稳定。

```{r rf-oob}
rf_model$prediction.error
```

```{r rf-predict}
rf_prob <- predict(rf_model, data = test_data)$predictions[, "setosa"]
rf_pred <- if_else(rf_prob >= 0.5, "setosa", "other") |> factor(levels = levels(test_data$target))

rf_metrics <- tibble(
  truth = test_data$target,
  .pred_setosa = rf_prob,
  .pred_class = rf_pred
) |>
  summarise(
    accuracy = yardstick::accuracy_vec(truth, .pred_class),
    roc_auc = yardstick::roc_auc_vec(truth, .pred_setosa, event_level = "first")
  )

rf_metrics
```

## 模型选择与对比

对比结果时，树模型强调可解释，随机森林强调泛化。

```{r model-compare}
comparison <- bind_rows(
  tree_metrics |> mutate(model = "Decision Tree"),
  rf_metrics |> mutate(model = "Random Forest")
) |>
  select(model, accuracy, roc_auc)

comparison
```

## 调参流程：mtry 与节点规模

```{r rf-tuning}
set.seed(2026)
tuning_grid <- expand.grid(
  mtry = c(1, 2, 3),
  min.node.size = c(1, 3, 5)
)

results <- lapply(seq_len(nrow(tuning_grid)), function(i) {
  params <- tuning_grid[i, ]
  model <- ranger(
    target ~ .,
    data = train_data,
    num.trees = 400,
    mtry = params$mtry,
    min.node.size = params$min.node.size,
    probability = TRUE
  )
  pred_prob <- predict(model, data = test_data)$predictions[, "setosa"]
  pred_class <- if_else(pred_prob >= 0.5, "setosa", "other") |> factor(levels = levels(test_data$target))

  tibble(
    mtry = params$mtry,
    min_node = params$min.node.size,
    accuracy = yardstick::accuracy_vec(test_data$target, pred_class),
    roc_auc = yardstick::roc_auc_vec(test_data$target, pred_prob, event_level = "first")
  )
})

bind_rows(results) |>
  arrange(desc(roc_auc))
```

## 可解释性与可视化

### 特征重要性

```{r rf-importance}
importance_df <- tibble(
  feature = names(rf_model$variable.importance),
  importance = rf_model$variable.importance
) |>
  arrange(desc(importance))

importance_df
```

```{r rf-importance-plot}
ggplot(importance_df, aes(x = reorder(feature, importance), y = importance)) +
  geom_col(fill = "#4f46e5", alpha = 0.85) +
  coord_flip() +
  labs(
    title = "随机森林特征重要性",
    x = NULL,
    y = "Permutation Importance"
  ) +
  theme_minimal(base_size = 12)
```

### 决策树规则解释

单棵树可直接给出清晰的“如果-那么”规则，适合对业务方解释。
随机森林适合提升预测，但需要用特征重要性或局部解释补充说明。

## 常见错误与优化

- **忽视类别不平衡**：应考虑分层抽样、调整阈值或设置 class weights。
- **树太深**：会过拟合，需控制 `min.node.size` 或树深度。
- **盲目追求高精度**：结合可解释性与业务成本。
- **只看特征重要性**：重要性不等于因果，需要结合领域知识。
- **未设置随机种子**：结果不可复现。
- **忽略 OOB 误差**：OOB 可用来快速诊断模型是否过拟合。

## 部署与复现建议

```{r rf-save}
model_path <- "models/rf-iris.rds"
if (!dir.exists("models")) dir.create("models")
saveRDS(rf_model, model_path)

loaded_model <- readRDS(model_path)
all.equal(rf_model$forest$num.trees, loaded_model$forest$num.trees)
```

```{r rf-cleanup}
if (file.exists("models/rf-iris.rds")) {
  file.remove("models/rf-iris.rds")
}
if (dir.exists("models")) {
  unlink("models", recursive = TRUE)
}
```

## 总结

决策树适合快速解释与沟通，随机森林适合追求稳健的预测表现。
在实际工作中，可以先用树模型建立基线，再用随机森林提升效果。